{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "\n",
    "In this notebook, we will load a previously trained model, explore the learned topics, and predict topics for a paper on arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from utils import scrape_arxiv_abstract\n",
    "from model import TopicModel\n",
    "from dataset import ArXivDataset\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build topic model\n",
    "\n",
    "To build a `TopicModel` object, we need to pass in as arguments the dataset used to create the model (to process new instances) and the model itself (to predict topics for the new instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic model\n",
    "model_path = \"../models/lda_n15_p5_r929_c38.1\"\n",
    "dataset_path = \"./processed_arxiv_data.obj\"\n",
    "model = TopicModel(model_path, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate topics\n",
    "\n",
    "Next, let us explore the different topics learned by the model so that we can assign understandable topic names to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.020*\"model\" + 0.012*\"_\" + 0.008*\"function\" + 0.008*\"theory\" + '\n",
      "  '0.007*\"case\" + 0.007*\"line\" + 0.006*\"new\" + 0.006*\"result\" + 0.005*\"study\" '\n",
      "  '+ 0.005*\"mass\"'),\n",
      " (1,\n",
      "  '0.013*\"network\" + 0.011*\"state\" + 0.009*\"system\" + 0.007*\"model\" + '\n",
      "  '0.007*\"effect\" + 0.007*\"quantum\" + 0.006*\"time\" + 0.006*\"qubit\" + '\n",
      "  '0.006*\"result\" + 0.006*\"entanglement\"'),\n",
      " (2,\n",
      "  '0.012*\"planet\" + 0.009*\"system\" + 0.008*\"state\" + 0.008*\"star\" + '\n",
      "  '0.007*\"low\" + 0.007*\"mass\" + 0.006*\"large\" + 0.006*\"effect\" + '\n",
      "  '0.005*\"galaxy\" + 0.005*\"atom\"'),\n",
      " (3,\n",
      "  '0.014*\"system\" + 0.010*\"function\" + 0.008*\"orbit\" + 0.007*\"result\" + '\n",
      "  '0.006*\"model\" + 0.006*\"energy\" + 0.005*\"solution\" + 0.005*\"order\" + '\n",
      "  '0.005*\"method\" + 0.005*\"binary\"'),\n",
      " (4,\n",
      "  '0.014*\"quantum\" + 0.013*\"state\" + 0.009*\"field\" + 0.007*\"system\" + '\n",
      "  '0.006*\"problem\" + 0.005*\"source\" + 0.005*\"theory\" + 0.005*\"equation\" + '\n",
      "  '0.005*\"non\" + 0.005*\"time\"'),\n",
      " (5,\n",
      "  '0.026*\"theory\" + 0.018*\"field\" + 0.010*\"gauge\" + 0.008*\"model\" + '\n",
      "  '0.007*\"result\" + 0.007*\"dimensional\" + 0.006*\"structure\" + 0.005*\"function\" '\n",
      "  '+ 0.005*\"mass\" + 0.005*\"dimension\"'),\n",
      " (6,\n",
      "  '0.010*\"vortex\" + 0.009*\"disk\" + 0.009*\"energy\" + 0.008*\"model\" + '\n",
      "  '0.006*\"result\" + 0.006*\"state\" + 0.005*\"field\" + 0.005*\"system\" + '\n",
      "  '0.005*\"age\" + 0.004*\"value\"'),\n",
      " (7,\n",
      "  '0.010*\"cluster\" + 0.009*\"energy\" + 0.008*\"galaxy\" + 0.008*\"model\" + '\n",
      "  '0.007*\"equation\" + 0.007*\"time\" + 0.007*\"mass\" + 0.006*\"density\" + '\n",
      "  '0.006*\"disk\" + 0.006*\"high\"'),\n",
      " (8,\n",
      "  '0.013*\"energy\" + 0.013*\"particle\" + 0.008*\"spin\" + 0.008*\"electron\" + '\n",
      "  '0.007*\"high\" + 0.007*\"field\" + 0.006*\"experiment\" + 0.006*\"result\" + '\n",
      "  '0.006*\"quantum\" + 0.005*\"model\"'),\n",
      " (9,\n",
      "  '0.022*\"model\" + 0.013*\"spin\" + 0.013*\"field\" + 0.011*\"state\" + '\n",
      "  '0.009*\"phase\" + 0.009*\"wave\" + 0.009*\"system\" + 0.008*\"temperature\" + '\n",
      "  '0.007*\"density\" + 0.007*\"transition\"'),\n",
      " (10,\n",
      "  '0.012*\"model\" + 0.008*\"method\" + 0.007*\"space\" + 0.007*\"problem\" + '\n",
      "  '0.007*\"function\" + 0.007*\"equation\" + 0.007*\"time\" + 0.006*\"number\" + '\n",
      "  '0.006*\"phase\" + 0.006*\"distribution\"'),\n",
      " (11,\n",
      "  '0.008*\"source\" + 0.008*\"result\" + 0.006*\"function\" + 0.006*\"paper\" + '\n",
      "  '0.006*\"pion\" + 0.005*\"space\" + 0.005*\"datum\" + 0.005*\"decay\" + '\n",
      "  '0.005*\"energy\" + 0.005*\"system\"'),\n",
      " (12,\n",
      "  '0.009*\"model\" + 0.008*\"star\" + 0.008*\"result\" + 0.007*\"source\" + '\n",
      "  '0.007*\"low\" + 0.006*\"structure\" + 0.006*\"band\" + 0.006*\"high\" + 0.005*\"_\" + '\n",
      "  '0.005*\"time\"'),\n",
      " (13,\n",
      "  '0.020*\"field\" + 0.015*\"magnetic\" + 0.007*\"phase\" + 0.007*\"result\" + '\n",
      "  '0.006*\"transition\" + 0.005*\"large\" + 0.005*\"electron\" + 0.005*\"effect\" + '\n",
      "  '0.005*\"category\" + 0.005*\"high\"'),\n",
      " (14,\n",
      "  '0.017*\"line\" + 0.014*\"emission\" + 0.011*\"galaxy\" + 0.011*\"model\" + '\n",
      "  '0.010*\"luminosity\" + 0.008*\"high\" + 0.007*\"source\" + 0.007*\"result\" + '\n",
      "  '0.007*\"measurement\" + 0.007*\"sample\"')]\n"
     ]
    }
   ],
   "source": [
    "# print topics\n",
    "pprint(model.topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some clusters that seem to refer to specific topics in machine learning. One of them is topic 7, which seems to direcly relate to sequential and time-series data. Another example is topic 10, which seems to be related to reinforcement learning.\n",
    "\n",
    "To make it easier to refer to these topic clusters, we will assign (tentative) names to each of them. Note that these names are subject to interpretation and are only assigned to help \"summarize\" each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural language processing',\n",
      "  '0.020*\"model\" + 0.012*\"_\" + 0.008*\"function\" + 0.008*\"theory\" + '\n",
      "  '0.007*\"case\" + 0.007*\"line\" + 0.006*\"new\" + 0.006*\"result\" + 0.005*\"study\" '\n",
      "  '+ 0.005*\"mass\"'),\n",
      " ('Probability + Inference',\n",
      "  '0.013*\"network\" + 0.011*\"state\" + 0.009*\"system\" + 0.007*\"model\" + '\n",
      "  '0.007*\"effect\" + 0.007*\"quantum\" + 0.006*\"time\" + 0.006*\"qubit\" + '\n",
      "  '0.006*\"result\" + 0.006*\"entanglement\"'),\n",
      " ('ML-related terms?',\n",
      "  '0.012*\"planet\" + 0.009*\"system\" + 0.008*\"state\" + 0.008*\"star\" + '\n",
      "  '0.007*\"low\" + 0.007*\"mass\" + 0.006*\"large\" + 0.006*\"effect\" + '\n",
      "  '0.005*\"galaxy\" + 0.005*\"atom\"'),\n",
      " ('Computer vision',\n",
      "  '0.014*\"system\" + 0.010*\"function\" + 0.008*\"orbit\" + 0.007*\"result\" + '\n",
      "  '0.006*\"model\" + 0.006*\"energy\" + 0.005*\"solution\" + 0.005*\"order\" + '\n",
      "  '0.005*\"method\" + 0.005*\"binary\"'),\n",
      " ('Recommendation',\n",
      "  '0.014*\"quantum\" + 0.013*\"state\" + 0.009*\"field\" + 0.007*\"system\" + '\n",
      "  '0.006*\"problem\" + 0.005*\"source\" + 0.005*\"theory\" + 0.005*\"equation\" + '\n",
      "  '0.005*\"non\" + 0.005*\"time\"'),\n",
      " ('Algorithms + Optimization',\n",
      "  '0.026*\"theory\" + 0.018*\"field\" + 0.010*\"gauge\" + 0.008*\"model\" + '\n",
      "  '0.007*\"result\" + 0.007*\"dimensional\" + 0.006*\"structure\" + 0.005*\"function\" '\n",
      "  '+ 0.005*\"mass\" + 0.005*\"dimension\"'),\n",
      " ('Deep learning',\n",
      "  '0.010*\"vortex\" + 0.009*\"disk\" + 0.009*\"energy\" + 0.008*\"model\" + '\n",
      "  '0.006*\"result\" + 0.006*\"state\" + 0.005*\"field\" + 0.005*\"system\" + '\n",
      "  '0.005*\"age\" + 0.004*\"value\"'),\n",
      " ('Sequences + Time series',\n",
      "  '0.010*\"cluster\" + 0.009*\"energy\" + 0.008*\"galaxy\" + 0.008*\"model\" + '\n",
      "  '0.007*\"equation\" + 0.007*\"time\" + 0.007*\"mass\" + 0.006*\"density\" + '\n",
      "  '0.006*\"disk\" + 0.006*\"high\"'),\n",
      " ('ML-related terms?',\n",
      "  '0.013*\"energy\" + 0.013*\"particle\" + 0.008*\"spin\" + 0.008*\"electron\" + '\n",
      "  '0.007*\"high\" + 0.007*\"field\" + 0.006*\"experiment\" + 0.006*\"result\" + '\n",
      "  '0.006*\"quantum\" + 0.005*\"model\"'),\n",
      " ('Estimation + Linear algebra?',\n",
      "  '0.022*\"model\" + 0.013*\"spin\" + 0.013*\"field\" + 0.011*\"state\" + '\n",
      "  '0.009*\"phase\" + 0.009*\"wave\" + 0.009*\"system\" + 0.008*\"temperature\" + '\n",
      "  '0.007*\"density\" + 0.007*\"transition\"'),\n",
      " ('Reinforcement learning',\n",
      "  '0.012*\"model\" + 0.008*\"method\" + 0.007*\"space\" + 0.007*\"problem\" + '\n",
      "  '0.007*\"function\" + 0.007*\"equation\" + 0.007*\"time\" + 0.006*\"number\" + '\n",
      "  '0.006*\"phase\" + 0.006*\"distribution\"'),\n",
      " ('Paper-related terms?',\n",
      "  '0.008*\"source\" + 0.008*\"result\" + 0.006*\"function\" + 0.006*\"paper\" + '\n",
      "  '0.006*\"pion\" + 0.005*\"space\" + 0.005*\"datum\" + 0.005*\"decay\" + '\n",
      "  '0.005*\"energy\" + 0.005*\"system\"'),\n",
      " ('Artificial intelligence',\n",
      "  '0.009*\"model\" + 0.008*\"star\" + 0.008*\"result\" + 0.007*\"source\" + '\n",
      "  '0.007*\"low\" + 0.006*\"structure\" + 0.006*\"band\" + 0.006*\"high\" + 0.005*\"_\" + '\n",
      "  '0.005*\"time\"'),\n",
      " ('Data mining',\n",
      "  '0.020*\"field\" + 0.015*\"magnetic\" + 0.007*\"phase\" + 0.007*\"result\" + '\n",
      "  '0.006*\"transition\" + 0.005*\"large\" + 0.005*\"electron\" + 0.005*\"effect\" + '\n",
      "  '0.005*\"category\" + 0.005*\"high\"'),\n",
      " ('Robotics',\n",
      "  '0.017*\"line\" + 0.014*\"emission\" + 0.011*\"galaxy\" + 0.011*\"model\" + '\n",
      "  '0.010*\"luminosity\" + 0.008*\"high\" + 0.007*\"source\" + 0.007*\"result\" + '\n",
      "  '0.007*\"measurement\" + 0.007*\"sample\"')]\n"
     ]
    }
   ],
   "source": [
    "# set topic names\n",
    "topic_names = [\n",
    "    \"Natural language processing\",\n",
    "    \"Probability + Inference\",\n",
    "    \"ML-related terms?\",\n",
    "    \"Computer vision\",\n",
    "    \"Recommendation\",\n",
    "    \"Algorithms + Optimization\",\n",
    "    \"Deep learning\",\n",
    "    \"Sequences + Time series\",\n",
    "    \"ML-related terms?\",\n",
    "    \"Estimation + Linear algebra?\",\n",
    "    \"Reinforcement learning\",\n",
    "    \"Paper-related terms?\",\n",
    "    \"Artificial intelligence\",\n",
    "    \"Data mining\",\n",
    "    \"Robotics\"\n",
    "]\n",
    "\n",
    "model.set_topic_names(topic_names)\n",
    "pprint(model.topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict topics for a paper\n",
    "\n",
    "Let us see how our model predicts a paper taken directly from arXiv. Using the `scrape_arxiv_abstract()` function, we can extract the title and the abstract of any paper on arXiv given its URL. Once scraped, this title and abstract can be passed into our topic model's `predict()` method.\n",
    "\n",
    "To illustrate, let us scrape the title and abstract from the seminal paper [\"Attention Is All You Need\" (2017)](https://arxiv.org/abs/1706.03762) and see what topics the model detects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# paper: \"Attention Is All You Need\" (Vaswani et al, 2017)\n",
    "paper_url = \"https://arxiv.org/abs/1706.03762\"\n",
    "text = scrape_arxiv_abstract(paper_url)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Reinforcement learning', 0.67122495),\n",
       " ('Artificial intelligence', 0.11870766),\n",
       " ('Natural language processing', 0.114291765)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions\n",
    "model.predict(text)[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
